import nltk
from nltk.tokenize import word_tokenize
str1 = "India is exporting $10 million software services to USA"
str1 = str1.lower()

#Word Tokenization
wt = word_tokenize(str1)
print(wt)
nltk.data.path

#sentence Tokenization
from nltk.tokenize import sent_tokenize
str2 = "India is exporting $10 million software to USA. Software is one of the most growing sector"
ws = sent_tokenize(str2)
print(ws)
ws[0]

#Frequency distribution
from nltk.probability import FreqDist
wt1 = word_tokenize(str2)
fdist = FreqDist(wt1)
fdist.most_common(2)

import matplotlib.pyplot as plt
fdist.plot(30,cumulative=False)
plt.show()

#part of speech(pos)
pos = nltk.pos_tag(wt)
pos

#Stop words
from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
print(stop_words)

filtered1 = []
for w in wt1:
    if w not in stop_words:
        filtered1.append(w)
print("Tokenized :",wt1)
print("Filtered :",filtered1)

#lemmatization (it needs context)
str3 = "I am runner running in the race as i love to run since i ran past years"
wt1 = word_tokenize(str3)
from nltk.stem.wordnet import WordNetLemmatizer
lem = WordNetLemmatizer()
lem_words = []
for w in wt1:
    lem_words.append(lem.lemmatize(w,'v'))
lem_words

#stemming
str3 = "connection connectivity connected connecting"
str3 = "I am a runner running in the race as i love to run since i ran past years"
from nltk.stem import PorterStemmer
wt = word_tokenize(str3)

ps = PorterStemmer()
stemmed_words=[]
for w in wt:
    stemmed_words.append(ps.stem(w))
stemmed_words

#COunt Vectorizer
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
import pandas as pd
cv1 = CountVectorizer()

x_traincv = cv1.fit_transform(["Hi How are you How are you doing","I am doing very very good","Wow that's awsome really awsome" ])

x_traincv_df = pd.DataFrame(x_traincv.toarray(),columns=list(cv1.get_feature_names()))

x_traincv_df

tf1 = TfidfVectorizer()
x_traintv = tf1.fit_transform(["Hi How are you How are you doing","I am doing very very good","Wow that's awsome really awsome" ])

x_traintv_df = pd.DataFrame(x_traintv.toarray(),columns=list(tf1.get_feature_names()))

x_traintv_df







